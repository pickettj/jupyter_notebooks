{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora Cleaning, Tokenizing, Pickling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/Enkidu/Documents/digital_humanities/python_routines/')\n",
    "\n",
    "import arabic_cleaning as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, os, glob, pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Globbing Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Build: Machine-Readable Central Asian Persian Texts\n",
    "Corpus of complete texts edited by others composed in early modern Transoxania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['khumuli', 'samarat', 'ikromcha', 'proshenie_k_general-gubernatory_ser721', 'rasskaz_praviteli_shahrisabz_ser724', 'prisoedineniia_samarkand_ser723', 'damla_abid_akhund_ser722', 'tarikh-i_jadida_tashkent_ser725', 'tuhfa-ahli-bukhara_ser25', 'darbandi_alexiii_coronation_ser728', 'tuhfa-i_taib_ser726'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_corpus_files = glob.glob(r'/Users/Enkidu/Box Sync/Notes/Digital Humanities/\\\n",
    "Corpora/machine_readable_persian_transoxania_texts//**/*.txt', recursive=True)\n",
    "\n",
    "trans_corpus = {}\n",
    "for longname in trans_corpus_files:\n",
    "    with open(longname) as f:\n",
    "        txt = f.read()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    trans_corpus[short[0]] = txt\n",
    "    \n",
    "    \n",
    "trans_corpus.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Build: Machine-Readable Indo-Persian Texts\n",
    "Corpus of complete texts edited by others composed in early modern India."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ratan_lal_tuhfa-i_dakkan_ser783', 'jilani_salar_al-intizam_ser789', 'qanuncha_adalat_ser788', 'dustur_al-amal_ser790', 'gawhar_khan_waqa-i_shaykh_dalil_ser796', 'hidayat-i_zururiyya_kotwali_ser791', 'muhammad_al-madrasi_minhaj_al-adala_ser801', 'shahjahanpuri_yadgar-i_makhan_lal_ser780', 'awrangabadi_gul-i_rana_ser794'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indo_corpus_files = glob.glob(r'/Users/Enkidu/Box Sync/Notes/Digital Humanities/\\\n",
    "Corpora/indo-persian_corpora//**/*.txt', recursive=True)\n",
    "\n",
    "indo_corpus = {}\n",
    "for longname in indo_corpus_files:\n",
    "    with open(longname) as f:\n",
    "        txt = f.read()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    indo_corpus[short[0]] = txt\n",
    "    \n",
    "#add in personally transcribed texts\n",
    "\n",
    "indo_man_files = glob.glob(r'/Users/Enkidu/Box Sync/Notes/Primary Sources/non-machine-readable_notes/Indian Manuscripts//**/*.txt', recursive=True)\n",
    "\n",
    "indo_man = {}\n",
    "for longname in indo_man_files:\n",
    "    with open(longname) as f:\n",
    "        txt = f.read()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    indo_man[short[0]] = txt\n",
    "    \n",
    "    \n",
    "#indo_corpus.keys()\n",
    "indo_man.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Build: Manuscript Notes\n",
    "Corpus based on partially transcribed manuscripts from early modern Transoxania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmr_files = glob.glob(r'/Users/Enkidu/Box Sync/Notes/\\\n",
    "Primary Sources/non-machine-readable_notes/bactriana_notes/*.txt')\n",
    "\n",
    "raw_notes_corpus = {}\n",
    "for longname in nmr_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    raw_notes_corpus[short[0]] = txt\n",
    "\n",
    "#Adding in MarkDown stage files, not yet converted to XML    \n",
    "md_files = glob.glob(r'/Users/Enkidu/Box Sync/Notes/\\\n",
    "Primary Sources/transcription_markdown_drafting_stage1/document_conversion_backlog/pre-parser_backlog/**/*.txt', recursive=True)\n",
    "\n",
    "md_notes_corpus = {}\n",
    "for longname in md_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    md_notes_corpus[short[0]] = txt\n",
    "\n",
    "#raw_notes_corpus.keys()\n",
    "#md_notes_corpus.keys()\n",
    "#md_notes_corpus['apsa_524']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Build: XML Documents\n",
    "Corpus based on transcribed XML documents early modern Transoxania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.5 and newer supports recursive **/ functionality, i.e. cycle through all subdirectories.\n",
    "\n",
    "xml_files = glob.glob(r'/Users/Enkidu/Box Sync/Notes/Primary Sources/xml_notes_stage2/**/*.xml', recursive=True)\n",
    "\n",
    "\n",
    "# For-loop through file names and build a dictionary of key (filename): value (text content)\n",
    "\n",
    "xml_corpus = {}\n",
    "for longname in xml_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    xml_corpus[short[0]] = txt\n",
    "\n",
    "#xml_corpus['TsGARUz_i126_1_601_6_ser187']\n",
    "#xml_corpus.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Defunct method: [creating an NLTK corpus](http://www.nltk.org/book/ch02.html#loading-your-own-corpus)\n",
    "\n",
    "```python\n",
    "\n",
    "os.chdir('/Users/Enkidu/Documents/digital_humanities/jupyter_notebooks')\n",
    "corpus_root = 'machine_readable_persian_transoxania_texts'\n",
    "turkestan_corpus = PlaintextCorpusReader(corpus_root, '.*')\n",
    "turkestan_corpus.fileids()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persian Literature Digital Corpus\n",
    "Massive corpus of Persian literature, pulled from Ganjur (http://ganjoor.net/) by Roshan (https://persdigumd.github.io/PDL/)\n",
    "\n",
    "*Corpus pre-cleaned, tokenized, and pickled from a separate script. (Cleaning takes a long time; and this corpus doesn't change very often, and so does not need to be re-run.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/Users/Enkidu/Box Sync/Notes/Digital Humanities/Corpora/corpus_scripts/persian_lit_toks.pkl', 'rb') \n",
    "\n",
    "pers_lit_toks = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pers_lit_toks.keys()\n",
    "#pers_lit_toks[\"hafez.masnavi\"][:50]\n",
    "#pers_lit_toks['ferdowsi.shahnameh']\n",
    "\n",
    "#type (pers_lit_toks['ferdowsi.shahnameh'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a function is pulled from an external file (arabic_cleaning.py). Previous method saved for posterity:\n",
    "\n",
    "```python\n",
    "clean_edited_i = {}\n",
    "for fn in raw_edited_corpus:\n",
    "    clean_edited_i[fn] = re.sub(r'ي', 'ی', raw_edited_corpus[fn])\n",
    "\n",
    "clean_edited = {}\n",
    "for fn in clean_edited_i:\n",
    "    clean_edited[fn] = re.sub(r'[^آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهس ي یی ]', '', clean_edited_i[fn])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning edited texts and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean edited texts\n",
    "\n",
    "## TO DO: figure out more efficient way of doing the for loop, add in ک swaps too\n",
    "\n",
    "#Transoxania Corpus (transcribed by others)\n",
    "clean_trans = {fn: ac.clean_document(doc) for fn, doc in trans_corpus.items()}\n",
    "\n",
    "#Indo Corpus (transcribed by others)\n",
    "clean_indo = {fn: ac.clean_document(doc) for fn, doc in indo_corpus.items()}\n",
    "\n",
    "#Manual Indo Corpus (transcribed by me)\n",
    "clean_indo_man = {fn: ac.clean_document(doc) for fn, doc in indo_man.items()}\n",
    "\n",
    "\n",
    "#XML-stage texts\n",
    "clean_xml = {fn: ac.clean_document(doc) for fn, doc in xml_corpus.items()}\n",
    "\n",
    "#Raw Notes\n",
    "clean_notes = {fn: ac.clean_document(doc) for fn, doc in raw_notes_corpus.items()}\n",
    "\n",
    "#Markdown Notes\n",
    "clean_markdown = {fn: ac.clean_document(doc) for fn, doc in md_notes_corpus.items()}\n",
    "\n",
    "\n",
    "#clean_trans['ikromcha'][:1000]\n",
    "#clean_trans['ikromcha'][:1000]\n",
    "\n",
    "#clean_markdown['apsa_76']\n",
    "\n",
    "#clean_xml['ser561']\n",
    "\n",
    "#clean_indo['mu_vol1'][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning XML documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dormant XML cleaning method using BeautifulSoup (still in use for Persian literature tokenization in separate script)*\n",
    "\n",
    "```python\n",
    "bstree = bs4.BeautifulSoup(clean_xml[\"ser561\"], 'lxml')\n",
    "\n",
    "\n",
    "print(bstree.get_text())\n",
    "\n",
    "clean_xml = {}\n",
    "for fn in raw_xml:\n",
    "    bstree = bs4.BeautifulSoup(raw_xml[fn], 'lxml')\n",
    "    clean_xml[fn] = bstree.get_text()\n",
    "    \n",
    "clean_xml['TsGARUZ_i126_1_1986_1_ser201']\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edited_toks = {}\n",
    "for (fn, txt) in clean_trans.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    edited_toks[fn] = toks\n",
    "\n",
    "indo_toks = {}\n",
    "for (fn, txt) in clean_indo.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    indo_toks[fn] = toks\n",
    "    \n",
    "indo_toks_man = {}\n",
    "for (fn, txt) in clean_indo_man.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    indo_toks_man[fn] = toks\n",
    "    \n",
    "\n",
    "notes_toks = {}\n",
    "for (fn, txt) in clean_notes.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    notes_toks[fn] = toks\n",
    "    \n",
    "markdown_toks = {}\n",
    "for (fn, txt) in clean_markdown.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    notes_toks[fn] = toks\n",
    "\n",
    "\n",
    "xml_toks = {}\n",
    "for (fn, txt) in clean_xml.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    xml_toks[fn] = toks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml_toks['TsGARUz_R-2678_ser184'][50:70]\n",
    "\n",
    "#notes_toks[\"jung_i_mahzar_va_rivayat_al_biruni_9767\"]\n",
    "\n",
    "#indo_toks['mu_vol1'][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pickled_refined_data/corpora.pkl\", \"wb\") as f:\n",
    "    pickle.dump((edited_toks, indo_toks, indo_toks_man, notes_toks, markdown_toks, xml_toks), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Corpuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-Corpuses:\n",
    "- (a) Indic Persian Mansucripts: corpus + manuscript notes\n",
    "- (b) Transoxania Manuscripts: corpus + manuscript notes\n",
    "- (c) Persian literature\n",
    "- (d) Documents (right now all together, regardless of location)\n",
    "\n",
    "Combined Corpora:\n",
    "- (i) History: a + b\n",
    "- (ii) Literature: c\n",
    "- (iii) Documents: d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging dictionaries: https://www.webucator.com/how-to/how-merge-dictionaries-python.cfm\n",
    "\n",
    "#Sub-Corpuses\n",
    "\n",
    "# (a) All Indic Manuscripts\n",
    "india_man_toks = {**indo_toks_man, **indo_toks}\n",
    "\n",
    "# (b) All Transoxania Manuscripts\n",
    "trans_man_toks = {**edited_toks, **notes_toks}\n",
    "\n",
    "# (c) All Documents\n",
    "doc_corpus_toks = {**xml_toks, **markdown_toks}\n",
    "\n",
    "\n",
    "#Corpora\n",
    "\n",
    "# (i) Historical Manuscripts\n",
    "hist_corpus_toks = {**india_man_toks, **trans_man_toks}\n",
    "\n",
    "\n",
    "# Meta-Corpus\n",
    "combined_corpus_toks = {**hist_corpus_toks, **doc_corpus_toks, **pers_lit_toks}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_corpus_toks['mu_vol1'][:50]\n",
    "#pers_lit_toks[\"hafez.masnavi\"][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling combined corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pickled_refined_data/meta_corpora.pkl\", \"wb\") as f:\n",
    "    pickle.dump((india_man_toks, trans_man_toks, doc_corpus_toks, hist_corpus_toks, combined_corpus_toks), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losing corpus hierarchy for simple token lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined Tokens (loses corpus text designation)\n",
    "\n",
    "doc_toks = []\n",
    "for (fn, text) in doc_corpus_toks.items():\n",
    "    doc_toks.extend(doc_corpus_toks[fn])\n",
    "    \n",
    "    \n",
    "hist_toks = []\n",
    "for (fn, text) in hist_corpus_toks.items():\n",
    "    hist_toks.extend(hist_corpus_toks[fn])\n",
    "    \n",
    "lit_toks = []\n",
    "for (fn, text) in pers_lit_toks.items():\n",
    "    lit_toks.extend(pers_lit_toks[fn])\n",
    "\n",
    "combined_toks = []\n",
    "for (fn, text) in combined_corpus_toks.items():\n",
    "    combined_toks.extend(combined_corpus_toks[fn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist_toks[100:125]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling Raw Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pickled_refined_data/raw_tokens.pkl\", \"wb\") as f:\n",
    "    pickle.dump((doc_toks, hist_toks, lit_toks, combined_toks), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
